{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import glob\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "from astropy.table import Table\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "### credit Jake Vanderplas, Adrian Price-Whelan\n",
    "\n",
    "The goal of dimensionality reduction is to find a model independent way of compressing the information without losing too much informaiton.\n",
    "\n",
    "The Principal Component Analysis (PCA) uses a linear transformation that project the dataset onto the top few eigenvectors components of the covariance matrix that contain the largest variance.\n",
    "\n",
    "Consider the following case of a 2-D Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0, 1., size=1024)\n",
    "y = np.random.normal(0, 0.1, size=x.size)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x, y, alpha=0.25, marker='.')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are bound to reduce this 2-D Gaussian, we better choose preserve $x_1$ as it contains a larger share of the variance of the distribution.\n",
    "\n",
    "If we flatten out the $x_2$ dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(x, y*0., alpha=0.25, marker='.', s= 3)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "The \"principal components\" of a dataset are the eigenvectors of the covariance matrix ordered by the magnitude of the variance.  With the first component being that has the largest variance. \n",
    "\n",
    "PCA then projects the original data onto a subspace defined by a subset of the eigenvectors. \n",
    "\n",
    "The covariance matrix is N by N where N is the number of the dimension of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"data/eigenvector.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('data/correlated_data.csv', delimiter=',')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at a test case in 3-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "trace = go.Scatter3d(\n",
    "    x=X[:,0],\n",
    "    y=X[:,1],\n",
    "    z=X[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color='blue',               # Set color to the z-values\n",
    "        colorscale='Viridis',   # Choose a colorscale\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the layout of the plot\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='x1',\n",
    "        yaxis_title='x2',\n",
    "        zaxis_title='x3'\n",
    "    ),\n",
    "    title=\"Interactive 3D Scatter Plot\"\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "fig.write_html(\"3-D_scatter.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's find the covariance matrix of this dataset and its eigenvector decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov = np.cov(X.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(Cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort by the eigenvalues and project the data onto the eigenbasis. The 0th feature is now the most informative, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = eig_vals.argsort()\n",
    "Y = X[:,idx] @ eig_vecs[idx] # projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1, 2, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "axes[0].plot(Y[:,0], Y[:,1], marker='.', alpha=0.5, linestyle='none')\n",
    "axes[1].plot(Y[:,1], Y[:,2], marker='.', alpha=0.5, linestyle='none')\n",
    "\n",
    "axes[0].set_xlim(-10, 10)\n",
    "axes[0].set_ylim(-10, 10)\n",
    "\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "axes[1].set_xlabel('$x_2$')\n",
    "axes[1].set_ylabel('$x_3$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to quantify how much variance or information is contained along each eigenvector is to compute the _cumulative explained variance_. This tells you, at a given eigenvector, what fraction the total variance is explained by the given eigenvector and all eigenvectors preceding it (with larger eigenvalues). Heres the CEV for the example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(eig_vals) / np.sum(eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the 0th eigenvector *alone* contains ~98% of the variance of the distribution, and 99.9% is contained with the first two. You have to decide what fraction of the explained variance is OK with you, and truncate the eigenvectors there. Here, if we arbitrarily say we want >95% of the explained variance, we would just keep the first eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example: Eigenfaces\n",
    "\n",
    "Here we're going to do an example of PCA with some more interesting, high-dimensional data: pictures of faces. Each image is 62 x 47, so if we treat each pixel as a feature, that's 2914 features *per image* or *per object*. We'll do a PCA on that large feature-space and see how many \"eigenvectors\" we need to explain 95% of the variance (i.e. do a pretty good job at representing faces).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_lfw_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the face image data\n",
    "faces = fetch_lfw_people(min_faces_per_person=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces.images.shape, faces.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample of what some of the images look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 8, figsize=(9, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(faces.images[i], cmap='Greys_r')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopefully you also recognize some of these people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the pixel \"flux\" values as if the are features / dimensions of the data. Here are a few projections of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2, 2, figsize=(8,8), sharex=True, sharey=True)\n",
    "\n",
    "axes[0,0].scatter(faces.images[:,0,0], faces.images[:,32,16], marker='.')\n",
    "axes[0,1].scatter(faces.images[:,11,21], faces.images[:,25,13], marker='.')\n",
    "axes[1,0].scatter(faces.images[:,13,41], faces.images[:,59,12], marker='.')\n",
    "axes[1,1].scatter(faces.images[:,2,20], faces.images[:,4,11], marker='.')\n",
    "\n",
    "axes[0,0].set_xlim(-5, 260)\n",
    "axes[0,0].set_ylim(-5, 260)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### as we can see, some pixels are definitely correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform a PCA on the image data, and keep the largest 256 eigenvectors (remember each eigenvectors has the same dimension as the orignal dataset). Here, we'll use the PCA implementation in `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(256)\n",
    "pca.fit(faces.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the top eigenfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 8, figsize=(9, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='Greys_r')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from being slightly terrifying, it's cool! You can see components needed for shading faces, making eyes darker, making noses wider, etc. Let's look at the cumulative explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative variance');\n",
    "plt.ylim(0, 1)\n",
    "plt.xscale('log', basex=2)\n",
    "plt.axvline(faces.images[0].size, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line is at 2914, the number of input features. How many eigenfaces do we need to use to preserve 95% of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(np.abs(np.cumsum(pca.explained_variance_ratio_) - 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 170! That's a big data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a few more faces here and project them to PCA components. Let's see how many components are needed for you to recognize those faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_faces = []\n",
    "for filename in glob.glob('data/faces/*.jpg'):\n",
    "    image_file = Image.open(filename)\n",
    "    image = image_file.convert('L')  # convert image to monochrome\n",
    "    image = np.array(image).astype(float)\n",
    "    other_faces.append(image)\n",
    "\n",
    "n_other = len(other_faces)\n",
    "other_faces = np.array(other_faces)\n",
    "other_faces = other_faces.reshape(n_other, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_face_data = np.vstack((faces.data, other_faces))\n",
    "image_shape = faces.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(n_components):\n",
    "    fig, ax = plt.subplots(2, 6, figsize=(16, 6),\n",
    "                           subplot_kw={'xticks':[], 'yticks':[]})\n",
    "\n",
    "    pca = PCA(n_components)\n",
    "    pca.fit(all_face_data)\n",
    "\n",
    "    for i in range(len(other_faces)):\n",
    "        components = pca.transform(other_faces[i].reshape(1, -1))\n",
    "        projected = pca.inverse_transform(components)\n",
    "        ax.flat[i].imshow(projected[0].reshape(image_shape), cmap='binary_r')\n",
    "\n",
    "    fig.suptitle('{}-dim reconstruction'.format(pca.n_components), fontsize=24)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_pca(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_pca(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_pca(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_pca(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_pca(384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite remarkable that our faces can well encapsulated in just a few hundred vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise: PCA of hand-written digits\n",
    "This is a classical machine learning dataset of hand-written digits in 8 by 8 grey pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, ax = plt.subplots(10, 10, figsize=(8, 8),\n",
    "                           subplot_kw=dict(xticks=[], yticks=[]))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        im = axi.imshow(data[i].reshape(8, 8), cmap='binary')\n",
    "        im.set_clim(0, 16)\n",
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's perform a PCA on these, report how many eigenvectors we need to capture 95\\% of the variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the most important 10 vectors in the 8 by 8 image space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the projections into the first two principal components, label each point by its digit (data.target). Which numbers are clustered together? In real life, which digits do you usually have trouble distinguishing? Is it the case here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "teaching/ast542/notebooks/Machine learning - intro - lecture notes-Copy1.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
